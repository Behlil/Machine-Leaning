{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! Let's break down the process of implementing simple linear regression with gradient descent step by step. We'll cover the intuition, equations, Python code, and demonstrate the results.\n",
    "\n",
    "## Simple Linear Regression with Gradient Descent\n",
    "\n",
    "### 1. Problem Formulation\n",
    "Suppose we have a small dataset where we are given the number of bedrooms in certain houses, and we want to predict the price of those houses. Our goal is to predict the price of any new house that enters our dataset.\n",
    "\n",
    "### 2. Mathematical Background\n",
    "Before diving into gradient descent, let's recap some key concepts:\n",
    "- **Linear Regression Equation**: The linear regression model predicts the target variable (e.g., house price) based on input features (e.g., number of bedrooms). The equation for simple linear regression is:\n",
    "\n",
    "    $$y = \\theta_0 + \\theta_1 \\cdot x$$\n",
    "\n",
    "    where:\n",
    "    - $y$ is the predicted price.\n",
    "    - $x* is the number of bedrooms.\n",
    "    - $\\theta_0$ is the y-intercept (bias term).\n",
    "    - $\\theta_1$ is the coefficient for the number of bedrooms.\n",
    "\n",
    "### 3. Gradient Descent\n",
    "Gradient descent is an optimization algorithm used to minimize the cost function (error) in machine learning models. Here's how it works step by step:\n",
    "\n",
    "#### 3.1. Initialize Parameters\n",
    "We start by initializing the model parameters \\(\\theta_0\\) and \\(\\theta_1\\) randomly.\n",
    "\n",
    "#### 3.2. Compute the Cost Function\n",
    "The cost function measures how well our model fits the data. For linear regression, the cost function (mean squared error) is:\n",
    "$$J(\\theta_0, \\theta_1) = \\frac{1}{2m} \\sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ m $ is the number of data points.\n",
    "- $h(x^{(i)})$ is the predicted value for the $i$-th data point.\n",
    "\n",
    "\n",
    "#### 3.3. Update Parameters\n",
    "We update the parameters using the gradient of the cost function:\n",
    "\n",
    "$$ \\theta_j := \\theta_j - \\alpha \\frac{\\partial J}{\\partial \\theta_j} $$\n",
    "\n",
    "where:\n",
    "- $\\alpha$ is the learning rate (step size).\n",
    "- $\\frac{\\partial J}{\\partial \\theta_j}$ represents the partial derivative with respect to $\\theta_j$.\n",
    "\n",
    "\n",
    "#### 3.4. Repeat Steps 3.2 and 3.3\n",
    "We iterate the above steps until convergence (when the cost function reaches a minimum).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
